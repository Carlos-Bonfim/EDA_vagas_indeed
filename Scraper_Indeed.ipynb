{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping para Análise de dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Web Scraping no Portal de vagas de emprego <a href=https://www.indeed.com.br/>Indeed</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importando os pacotes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os pacotes a serem utilizados serão:\n",
    "- `BeautifulSoup` e `selenium` para *web scraping*\n",
    "- `numpy` e `pandas` para manipulação de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando os pacotes\n",
    "import bs4\n",
    "import selenium\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos utilizar o `Chromedriver` para o processo de extração. O arquivo após baixado, neste <a href=https://sites.google.com/a/chromium.org/chromedriver/home>link</a>,  deve ser descompactado no mesmo local deste `jupyter notebook`.<br>\n",
    "Quando criarmos o driver de conexão, será aberta outra janela do nevegador e deve-se manter aberta durante todo o processo de extração."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criando o driver de conexão\n",
    "driver = webdriver.Chrome(\"./chromedriver\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos criar um dataframe para receber os dados de nosso interesse, como *Título*, *Localidade*, *Empresa*, *Salario*, *Tipo de pesquisa* e *Descrição*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cria o dataframe para receber os dados\n",
    "dados = pd.DataFrame(columns=['Titulo', 'Localidade', 'Empresa', 'Classificacao', 'Salario', 'Dias_publicado', \n",
    "                              'Tipo_Pesquisa', 'Desc', 'Termo_pesq'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos realizar a pesquisa no site e pra isso, seguimos foram seguidos os seguintes passos:\n",
    "1. Acesso ao site: https://www.indeed.com.br\n",
    "2. Seleção dos filtros de pesquisa de vagas com o termo *Data Science\" e localidade \"Brasil\"\n",
    "3. Copiamos a url gerada na barra de endereços do navegador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criando um objeto para o termo\n",
    "termo_pesquisa = [\"data+science\", \"business+intelligence\", \"machine+learning\", \"analista+bi\", \n",
    "                  \"cientista+dados\", \"analista+dados\"]\n",
    "\n",
    "# criando um objeto para a localidade\n",
    "local = \"brasil\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Então vamos criar a função de busca no site, buscando pelas primeiras 20 páginas de cada termo buscado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in termo_pesquisa:\n",
    "\n",
    "    for i in range(0,35,1):\n",
    "        \n",
    "        driver.get(f\"https://www.indeed.com.br/jobs?q={j}&l={local}&start={str(i)}\")\n",
    "\n",
    "        time.sleep(6)\n",
    "    \n",
    "        driver.implicitly_wait(14)\n",
    "\n",
    "        \n",
    "\n",
    "        # criando um loop para os resultados do scraping\n",
    "        for vaga in driver.find_elements_by_class_name(\"result\"):\n",
    "\n",
    "            # coletando o elemento HTML interno\n",
    "            result_html = vaga.get_attribute('innerHTML')\n",
    "\n",
    "            # parser do código HTML\n",
    "            soup = BeautifulSoup(result_html, 'html.parser')\n",
    "\n",
    "            # Buscamos as tags para análise. \n",
    "            # Usaremos blocos try/except para evitar erros na execução, no caso de ua informação não estar disponível na vaga\n",
    "\n",
    "            # buscando pelo título da vaga\n",
    "            try:\n",
    "                title = soup.find(\"a\", class_ = \"jobtitle\").text.replace('\\n', '').strip()\n",
    "            except:\n",
    "                title = 'None'\n",
    "\n",
    "            # buscando pelo localidade\n",
    "            try:\n",
    "                location = soup.find(class_ = \"location\").text\n",
    "            except:\n",
    "                location = 'None'\n",
    "\n",
    "            # buscando pela empresa\n",
    "            try:\n",
    "                company = soup.find(class_ = \"company\").text.replace('\\n', '').strip()\n",
    "            except:\n",
    "                company = 'None'\n",
    "            \n",
    "            # buscando pela Classificação\n",
    "            try:\n",
    "                rating = soup.find(class_=\"ratingNumber\").text.replace('\\n', '').strip()\n",
    "            except:\n",
    "                rating = 'None'\n",
    "\n",
    "            # buscando pelo salário\n",
    "            try:\n",
    "                salary = soup.find(class_ = \"salary\").text.replace('\\n', '').strip()\n",
    "            except:\n",
    "                salary = 'None'\n",
    "                \n",
    "            # buscando pela quantidade de dias da publicação\n",
    "            try:\n",
    "                aging = soup.find(class_=\"date\").text.strip()\n",
    "            except:\n",
    "                aging = 'None'\n",
    "\n",
    "            # buscando pelo tipo de pesquisa (orgânica ou patrocinada)\n",
    "            try:\n",
    "                sponsored = soup.find(\"a\", class_ = \"sponsoredGray\").text\n",
    "                sponsored = \"Sponsored\"\n",
    "            except:\n",
    "                sponsored = 'Organic'\n",
    "\n",
    "            # buscando pelo sumário\n",
    "            sum_div = vaga.find_elements_by_class_name(\"summary\")[0]\n",
    "            try:\n",
    "                sum_div.click()\n",
    "            except:\n",
    "                close_button = driver.find_elements_by_class_name('popover-x-button-close')[0]\n",
    "                close_button.click()\n",
    "                sum_div.click()\n",
    "\n",
    "            # buscando pelo descrição da vaga\n",
    "            job_desc = driver.find_element_by_id('vjs-desc').text\n",
    "\n",
    "            # gravando o resultado em nosso dataframe\n",
    "            dados = dados.append({\"Titulo\":title, \n",
    "                                  \"Localidade\":location, \n",
    "                                  \"Empresa\":company, \n",
    "                                  \"Classificacao\": rating, \n",
    "                                  \"Salario\":salary, \n",
    "                                  \"Dias_publicado\": aging, \n",
    "                                  \"Tipo_Pesquisa\":sponsored, \n",
    "                                  \"Desc\":job_desc, \n",
    "                                  \"Termo_pesq\":j},\n",
    "                                 ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos olhar a dimensão do dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O dataframe possui:\n",
      "3150 linhas e 9 colunas\n"
     ]
    }
   ],
   "source": [
    "# exibindo a quantidade de linhas e colunas\n",
    "print(f'O dataframe possui:\\n{dados.shape[0]} linhas e {dados.shape[1]} colunas')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Legal! Agora temos dados para trabalhar, mas antes vamos salvar uma cópia em disco."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# salvando os dados em disco\n",
    "dados.to_csv('resultado/dados.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
